{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b288a91c",
   "metadata": {},
   "source": [
    "### SOP EVALUATION USING ML ALGORITHMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a1eb5",
   "metadata": {},
   "source": [
    "##### Reading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac4d360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('SOPs Dataset.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66253206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 1</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sno</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To the Admissions Committee of University of B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Committee of the University of Toron...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0  Unnamed: 1                                         Unnamed: 2  \\\n",
       "0        NaN         NaN                                                NaN   \n",
       "1        Sno         NaN                                                SOP   \n",
       "2        NaN         NaN                                                NaN   \n",
       "3          1         NaN  To the Admissions Committee of University of B...   \n",
       "4          2         NaN  Admission Committee of the University of Toron...   \n",
       "\n",
       "   Unnamed: 3 Unnamed: 4  \n",
       "0         NaN        NaN  \n",
       "1         NaN      Label  \n",
       "2         NaN        NaN  \n",
       "3         NaN        NaN  \n",
       "4         NaN        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20acb136",
   "metadata": {},
   "source": [
    "##### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f9d3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>SOP</th>\n",
       "      <th>Column4</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sno</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>To the Admissions Committee of University of B...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Admission Committee of the University of Toron...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>796</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dear Admissions Committee,\\n\\nI am Nisha Gupta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dear Admissions Committee,\\n\\nI am Anirudh Kap...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dear Admissions Committee,\\n\\nI am Priyanka Pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dear Admissions Committee,\\n\\nI am Arjun Mehta...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dear Admissions Committee,\\n\\nI am Meera Sharm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>821 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column1  Column2                                                SOP  \\\n",
       "0       NaN      NaN                                                NaN   \n",
       "1       Sno      NaN                                                SOP   \n",
       "2       NaN      NaN                                                NaN   \n",
       "3         1      NaN  To the Admissions Committee of University of B...   \n",
       "4         2      NaN  Admission Committee of the University of Toron...   \n",
       "..      ...      ...                                                ...   \n",
       "816     796      NaN  Dear Admissions Committee,\\n\\nI am Nisha Gupta...   \n",
       "817     797      NaN  Dear Admissions Committee,\\n\\nI am Anirudh Kap...   \n",
       "818     798      NaN  Dear Admissions Committee,\\n\\nI am Priyanka Pa...   \n",
       "819     799      NaN  Dear Admissions Committee,\\n\\nI am Arjun Mehta...   \n",
       "820     800      NaN  Dear Admissions Committee,\\n\\nI am Meera Sharm...   \n",
       "\n",
       "     Column4  Label  \n",
       "0        NaN    NaN  \n",
       "1        NaN  Label  \n",
       "2        NaN    NaN  \n",
       "3        NaN    NaN  \n",
       "4        NaN    NaN  \n",
       "..       ...    ...  \n",
       "816      NaN      1  \n",
       "817      NaN      1  \n",
       "818      NaN      1  \n",
       "819      NaN      1  \n",
       "820      NaN      1  \n",
       "\n",
       "[821 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "column_labels = ['Column1', 'Column2', 'SOP','Column4','Label']\n",
    "df.columns = column_labels\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2ce29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOP</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SOP</td>\n",
       "      <td>Label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To the Admissions Committee of University of B...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Admission Committee of the University of Toron...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>Dear Admissions Committee,\\n\\nI am Nisha Gupta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>Dear Admissions Committee,\\n\\nI am Anirudh Kap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>Dear Admissions Committee,\\n\\nI am Priyanka Pa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>Dear Admissions Committee,\\n\\nI am Arjun Mehta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Dear Admissions Committee,\\n\\nI am Meera Sharm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>821 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   SOP  Label\n",
       "0                                                  NaN    NaN\n",
       "1                                                  SOP  Label\n",
       "2                                                  NaN    NaN\n",
       "3    To the Admissions Committee of University of B...    NaN\n",
       "4    Admission Committee of the University of Toron...    NaN\n",
       "..                                                 ...    ...\n",
       "816  Dear Admissions Committee,\\n\\nI am Nisha Gupta...      1\n",
       "817  Dear Admissions Committee,\\n\\nI am Anirudh Kap...      1\n",
       "818  Dear Admissions Committee,\\n\\nI am Priyanka Pa...      1\n",
       "819  Dear Admissions Committee,\\n\\nI am Arjun Mehta...      1\n",
       "820  Dear Admissions Committee,\\n\\nI am Meera Sharm...      1\n",
       "\n",
       "[821 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropping unwanted columns and displaying the dataset\n",
    "columns_to_drop = ['Column1', 'Column2', 'Column4']\n",
    "df.drop(columns=columns_to_drop, inplace=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64a87ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOP</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To the Admissions Committee of University of B...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Admission Committee of the University of Toron...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dear Admissions Committee of McGill University...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>To the Esteemed Admissions Panel of McMaster U...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dear Admissions Committee of the University of...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 SOP Label\n",
       "3  To the Admissions Committee of University of B...   NaN\n",
       "4  Admission Committee of the University of Toron...   NaN\n",
       "5  Dear Admissions Committee of McGill University...   NaN\n",
       "6  To the Esteemed Admissions Panel of McMaster U...   NaN\n",
       "7  Dear Admissions Committee of the University of...   NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the first 3 since it has no values\n",
    "df = df.drop(df.index[:3])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd1061de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOP</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To the Admissions Committee of University of B...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Admission Committee of the University of Toron...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Admissions Committee of McGill University...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To the Esteemed Admissions Panel of McMaster U...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Admissions Committee of the University of...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 SOP Label\n",
       "0  To the Admissions Committee of University of B...   NaN\n",
       "1  Admission Committee of the University of Toron...   NaN\n",
       "2  Dear Admissions Committee of McGill University...   NaN\n",
       "3  To the Esteemed Admissions Panel of McMaster U...   NaN\n",
       "4  Dear Admissions Committee of the University of...   NaN"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the index to 0\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f6ee3fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOP</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To the Admissions Committee of University of B...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Admission Committee of the University of Toron...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Admissions Committee of McGill University...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To the Esteemed Admissions Panel of McMaster U...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Admissions Committee of the University of...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>To the Respected Admissions Board of the Unive...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Dear Admissions Committee at the University of...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>To the Esteemed Admissions Panel of McMaster U...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dear Admissions Board of Queen's University,\\n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>To the Admission Committee of the University o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 SOP  Label\n",
       "0  To the Admissions Committee of University of B...    NaN\n",
       "1  Admission Committee of the University of Toron...    NaN\n",
       "2  Dear Admissions Committee of McGill University...    NaN\n",
       "3  To the Esteemed Admissions Panel of McMaster U...    NaN\n",
       "4  Dear Admissions Committee of the University of...    NaN\n",
       "5  To the Respected Admissions Board of the Unive...    NaN\n",
       "6  Dear Admissions Committee at the University of...    NaN\n",
       "7  To the Esteemed Admissions Panel of McMaster U...    NaN\n",
       "8  Dear Admissions Board of Queen's University,\\n...    NaN\n",
       "9  To the Admission Committee of the University o...    NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all 1's with 0 in the 'Label' column\n",
    "df['Label'] = df['Label'].replace(1, 0)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "148316e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOP</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>To the Admissions Committee of University of B...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Admission Committee of the University of Toron...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear Admissions Committee of McGill University...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To the Esteemed Admissions Panel of McMaster U...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear Admissions Committee of the University of...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>To the Visa Officer,\\nCanada High Commission, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>To the Visa Officer,\\nCanada High Commission, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>To the Visa Officer,\\nCanada High Commission, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>To the Visa Officer,\\nCanada High Commission, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>To the Visa Officer,\\nCanada High Commission, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  SOP  Label\n",
       "0   To the Admissions Committee of University of B...    1.0\n",
       "1   Admission Committee of the University of Toron...    1.0\n",
       "2   Dear Admissions Committee of McGill University...    1.0\n",
       "3   To the Esteemed Admissions Panel of McMaster U...    1.0\n",
       "4   Dear Admissions Committee of the University of...    1.0\n",
       "..                                                ...    ...\n",
       "95  To the Visa Officer,\\nCanada High Commission, ...    0.0\n",
       "96  To the Visa Officer,\\nCanada High Commission, ...    0.0\n",
       "97  To the Visa Officer,\\nCanada High Commission, ...    0.0\n",
       "98  To the Visa Officer,\\nCanada High Commission, ...    0.0\n",
       "99  To the Visa Officer,\\nCanada High Commission, ...    0.0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all NaN values with 1 in the 'Label' column\n",
    "df['Label'].fillna(1, inplace=True)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b45d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null values in the 'SOP' column\n",
    "df = df.dropna(subset=['SOP'])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de4f1dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(804, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daa38c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SOP</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>804 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SOP  Label\n",
       "0    False  False\n",
       "1    False  False\n",
       "2    False  False\n",
       "3    False  False\n",
       "4    False  False\n",
       "..     ...    ...\n",
       "799  False  False\n",
       "800  False  False\n",
       "801  False  False\n",
       "802  False  False\n",
       "803  False  False\n",
       "\n",
       "[804 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "483f2937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values and printing columns with null values and their respective counts\n",
    "null_values = df.isnull().sum()\n",
    "for column, count in null_values.items():\n",
    "    if count > 0:\n",
    "        print(f'Column: {column}, Null Count: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68a38b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     630\n",
       "0.0     173\n",
       "11.0      1\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the dataset is biased or not\n",
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26eee10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    631\n",
       "0.0    173\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing one label 11.0 with 1.0\n",
    "df['Label'] = df['Label'].replace([11.0],1.0)\n",
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "911dca17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    631\n",
       "0.0    173\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38585530",
   "metadata": {},
   "source": [
    "Here we can understand that the count of approved SOP's are so high in number when compared to those which are rejected. \n",
    "Which creates a bias problem in the dataset.\n",
    "So we need to perform augmentation to balance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e9989",
   "metadata": {},
   "source": [
    "##### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "399b9a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\roysi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Function to replace rejected SOP words with its synonyms and create augmented SOP\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def synonym_replacement(text, n=1):\n",
    "    words = text.split()\n",
    "    augmented_texts = []\n",
    "    for _ in range(n):\n",
    "        augmented_words = []\n",
    "        for word in words:\n",
    "            synonyms = get_synonyms(word)\n",
    "            if synonyms:\n",
    "                synonym = random.choice(synonyms)\n",
    "                augmented_words.append(synonym)\n",
    "            else:\n",
    "                augmented_words.append(word)\n",
    "        augmented_text = ' '.join(augmented_words)\n",
    "        augmented_texts.append(augmented_text)\n",
    "    return str(augmented_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e36b6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roysi\\AppData\\Local\\Temp\\ipykernel_11536\\264420576.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rejected_sop[\"SOP\"] = aug_texts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(977, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1st iteration for augmentation SOP's with label 0\n",
    "rejected_sop = df[df['Label'] == 0]\n",
    "\n",
    "aug_texts = []\n",
    "for text in rejected_sop[\"SOP\"]:\n",
    "    aug_text = synonym_replacement(text)\n",
    "    aug_texts.append(aug_text)\n",
    "\n",
    "rejected_sop[\"SOP\"] = aug_texts\n",
    "rejected_sop.head()\n",
    "\n",
    "df = pd.concat([df, rejected_sop], ignore_index=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26719e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           SOP  \\\n",
      "79  ['To the visa Officer, Canada high Commission, novel Delhi. SUBJECT: AN diligence for study permit observe Officer, My epithet exist Vivek Malhotra, and ane represent vitamin_A network designer astatine TC with A passion for integrate engineering into healthcare. while my faculty_member carrying_out stomach atomic_number_85 68%, my IELTS mark of 6.0 have alas fall short_circuit of the threshold, run to my application for the knight_bachelor of scientific_discipline IN health IP atomic_number_85 the university of the Fraser valley be non accepted. disdain this setback, my dedication to the subject_field of wellness information_science remain steadfast. My professional experience At TCS, develop secure and efficient network root for healthcare providers, have prepare Pine_Tree_State for the dispute and chance in health informatics. atomic_number_53 Artium_Magister determine to improve my nomenclature proficiency and reapply for the program, vitamin_A i believe the university of the Fraser valley cost the nonpareil place to foster my growth in this active field. The program’s comprehensive_examination program and the university’s inscription to student success resonate with my educational aspirations. i plan to continue heighten my skill and impart to project astatine Trusteeship_Council that line_up with health information_processing until ane encounter the necessity for this prize program. I be aspirant for A future opportunity to articulation the university of the Fraser valley and bring_in my electric_potential in health informatics.']   \n",
      "\n",
      "    Label  \n",
      "79    0.0  \n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    print(rejected_sop.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51f79f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the index\n",
    "df = df.reset_index(level=0)\n",
    "df['index'] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ea01738",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\roysi\\AppData\\Local\\Temp\\ipykernel_11536\\4275085621.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  rejected_sop_1[\"SOP\"] = texts\n"
     ]
    }
   ],
   "source": [
    "#2nd iteration for augmentation the SOP's with label 0\n",
    "rejected_sop_1 = df[df['Label'] == 0] \n",
    "rejected_sop_1.SOP.count()\n",
    "\n",
    "texts = []\n",
    "for text in rejected_sop_1[\"SOP\"]:\n",
    "    aug_text = synonym_replacement(str(text))\n",
    "    texts.append(aug_text)\n",
    "\n",
    "rejected_sop_1[\"SOP\"] = texts\n",
    "rejected_sop.head()\n",
    "\n",
    "df = pd.concat([df, rejected_sop], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a4a03cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending the augmented SOP's to the dataframe\n",
    "new_sop =[]\n",
    "for row in df.SOP:\n",
    "    if type(row) == str:\n",
    "        new_sop.append(row)\n",
    "    else:\n",
    "        concatenated_series = pd.concat(row)\n",
    "        new_sop.append(', '.join(concatenated_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "902e4b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    631\n",
       "0.0    519\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total SOP Count after augmentation\n",
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68083148",
   "metadata": {},
   "source": [
    "##### NLP Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b39a564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99ce83c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15075892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation's\n",
    "def remove_punctuations(text):\n",
    "    punctuations = string.punctuation\n",
    "    return text.translate(str.maketrans('','',punctuations))\n",
    "df['SOP'] = df['SOP'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "761a5fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Clean_sop'] = df['SOP'].apply(lambda x: remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a992e2be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1145    heartfelt entrancefee Committee I be Nisha Gup...\n",
       "1146    dear access Committee ace constitute Anirudh K...\n",
       "1147    devout admission Committee I be Priyanka Patel...\n",
       "1148    honey admission Committee single be Arjun Meht...\n",
       "1149    dearly admission Committee I be Meera Sharma A...\n",
       "Name: Clean_sop, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Clean_sop.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "867efde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07902f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "150728a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"you're\", 'she', 'than', 'am', 'over', 'so', 'up', \"should've\", 'why', 'our', 'below', 'no', 'with', 'each', 's', 'wasn', 'who', 'didn', 'y', \"isn't\", 'most', 'out', 'myself', 'can', 'should', 'the', 'doing', 'an', 'just', 'being', 't', 'themselves', 'any', 'some', \"mightn't\", \"that'll\", 'him', 'because', 'their', 'his', 'does', 'hasn', \"won't\", 'a', 'to', 'shan', 'against', 'had', 'from', 'weren', \"she's\", 'into', 'theirs', 'where', 'after', 'very', 'too', 'about', 'won', 'you', 'how', 'under', 'be', 'couldn', 'hadn', 'ours', 'are', 'mightn', 'aren', 'whom', \"shan't\", 'it', 'there', 'm', 'll', 'when', 'd', \"couldn't\", \"don't\", 'but', \"doesn't\", \"you've\", 'this', \"needn't\", \"weren't\", 'been', 'my', 'itself', 'yourselves', 'as', 'her', 'before', 'that', 'these', 'once', 'for', 'same', 'those', \"hasn't\", \"shouldn't\", \"wasn't\", 'me', 'if', 'which', 'further', 'don', 'on', 'nor', \"hadn't\", 'mustn', 'i', 'own', 've', 'having', 'more', 'shouldn', 'isn', \"aren't\", 'is', 'few', 'what', 'hers', 'or', 'other', 'ourselves', \"it's\", 'herself', 'off', 'all', 'was', 'not', 'in', 'we', 'such', \"mustn't\", 'here', 're', 'did', 'has', 'haven', \"wouldn't\", 'until', 'now', 'ain', 'were', 'will', 'and', 'again', \"haven't\", 'they', 'then', \"you'll\", 'while', \"you'd\", 'your', 'he', 'both', 'its', 'only', 'himself', 'them', 'above', 'o', 'of', 'by', 'ma', 'through', 'do', 'wouldn', 'yourself', 'needn', 'down', 'during', 'have', 'doesn', 'yours', \"didn't\", 'at', 'between'}\n"
     ]
    }
   ],
   "source": [
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3807db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join(word for word in text.split() if word not in STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26ba44c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Clean_sop\"] = df[\"Clean_sop\"].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "34a7dcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SOP</th>\n",
       "      <th>Label</th>\n",
       "      <th>Clean_sop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"heartfelt entrance_fee Committee, I be Nisha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>heartfelt entrancefee Committee I Nisha Gupta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"dear access Committee, ace constitute Anirud...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear access Committee ace constitute Anirudh K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"devout admission Committee, I be Priyanka Pa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>devout admission Committee I Priyanka Patel ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"honey admission Committee, single be Arjun M...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>honey admission Committee single Arjun Mehta A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"dearly admission Committee, I be Meera Sharm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dearly admission Committee I Meera Sharma A gi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                                SOP  Label  \\\n",
       "1145    NaN  [\"heartfelt entrance_fee Committee, I be Nisha...    0.0   \n",
       "1146    NaN  [\"dear access Committee, ace constitute Anirud...    0.0   \n",
       "1147    NaN  [\"devout admission Committee, I be Priyanka Pa...    0.0   \n",
       "1148    NaN  [\"honey admission Committee, single be Arjun M...    0.0   \n",
       "1149    NaN  [\"dearly admission Committee, I be Meera Sharm...    0.0   \n",
       "\n",
       "                                              Clean_sop  \n",
       "1145  heartfelt entrancefee Committee I Nisha Gupta ...  \n",
       "1146  dear access Committee ace constitute Anirudh K...  \n",
       "1147  devout admission Committee I Priyanka Patel ad...  \n",
       "1148  honey admission Committee single Arjun Mehta A...  \n",
       "1149  dearly admission Committee I Meera Sharma A gi...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "571381cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower casing the SOP\n",
    "df['Clean_sop'] = df['Clean_sop'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "58534e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SOP</th>\n",
       "      <th>Label</th>\n",
       "      <th>Clean_sop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"heartfelt entrance_fee Committee, I be Nisha...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>heartfelt entrancefee committee i nisha gupta ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1146</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"dear access Committee, ace constitute Anirud...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear access committee ace constitute anirudh k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"devout admission Committee, I be Priyanka Pa...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>devout admission committee i priyanka patel ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"honey admission Committee, single be Arjun M...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>honey admission committee single arjun mehta a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"dearly admission Committee, I be Meera Sharm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dearly admission committee i meera sharma a gi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                                SOP  Label  \\\n",
       "1145    NaN  [\"heartfelt entrance_fee Committee, I be Nisha...    0.0   \n",
       "1146    NaN  [\"dear access Committee, ace constitute Anirud...    0.0   \n",
       "1147    NaN  [\"devout admission Committee, I be Priyanka Pa...    0.0   \n",
       "1148    NaN  [\"honey admission Committee, single be Arjun M...    0.0   \n",
       "1149    NaN  [\"dearly admission Committee, I be Meera Sharm...    0.0   \n",
       "\n",
       "                                              Clean_sop  \n",
       "1145  heartfelt entrancefee committee i nisha gupta ...  \n",
       "1146  dear access committee ace constitute anirudh k...  \n",
       "1147  devout admission committee i priyanka patel ad...  \n",
       "1148  honey admission committee single arjun mehta a...  \n",
       "1149  dearly admission committee i meera sharma a gi...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eefca5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\roysi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [to, admissions, committee, university, britis...\n",
      "1       [admission, committee, university, toronto, gr...\n",
      "2       [dear, admissions, committee, mcgill, universi...\n",
      "3       [to, esteemed, admissions, panel, mcmaster, un...\n",
      "4       [dear, admissions, committee, university, toro...\n",
      "                              ...                        \n",
      "1145    [heartfelt, entrancefee, committee, i, nisha, ...\n",
      "1146    [dear, access, committee, ace, constitute, ani...\n",
      "1147    [devout, admission, committee, i, priyanka, pa...\n",
      "1148    [honey, admission, committee, single, arjun, m...\n",
      "1149    [dearly, admission, committee, i, meera, sharm...\n",
      "Name: tokenized_text, Length: 1150, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the SOP text\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "df['tokenized_text'] = df['Clean_sop'].apply(lambda x: word_tokenize(x))\n",
    "print(df['tokenized_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cde9884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9987c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing the tokenized word with appropriate Parts Of Speech tagging \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to perform POS tagging and lemmatization\n",
    "def pos_mapping_lemmatization(text):\n",
    "    pos_tags = pos_tag(text)\n",
    "    \n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tags:\n",
    "        if tag.startswith('N'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('V'):\n",
    "            pos = 'v'\n",
    "        elif tag.startswith('R'):\n",
    "            pos = 'r'\n",
    "        else:\n",
    "            pos = 'n'\n",
    "        \n",
    "        lemma = lemmatizer.lemmatize(word, pos)\n",
    "        lemmatized_words.append(lemma)\n",
    "    \n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce84375a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SOP</th>\n",
       "      <th>Label</th>\n",
       "      <th>Clean_sop</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>To the Admissions Committee of University of B...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>to admissions committee university british col...</td>\n",
       "      <td>[to, admissions, committee, university, britis...</td>\n",
       "      <td>to admission committee university british colu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Admission Committee of the University of Toron...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>admission committee university toronto greetin...</td>\n",
       "      <td>[admission, committee, university, toronto, gr...</td>\n",
       "      <td>admission committee university toronto greetin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Dear Admissions Committee of McGill University...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>dear admissions committee mcgill university my...</td>\n",
       "      <td>[dear, admissions, committee, mcgill, universi...</td>\n",
       "      <td>dear admission committee mcgill university my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>To the Esteemed Admissions Panel of McMaster U...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>to esteemed admissions panel mcmaster universi...</td>\n",
       "      <td>[to, esteemed, admissions, panel, mcmaster, un...</td>\n",
       "      <td>to esteem admission panel mcmaster university ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Dear Admissions Committee of the University of...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>dear admissions committee university toronto m...</td>\n",
       "      <td>[dear, admissions, committee, university, toro...</td>\n",
       "      <td>dear admission committee university toronto my...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                SOP  Label  \\\n",
       "0    0.0  To the Admissions Committee of University of B...    1.0   \n",
       "1    1.0  Admission Committee of the University of Toron...    1.0   \n",
       "2    2.0  Dear Admissions Committee of McGill University...    1.0   \n",
       "3    3.0  To the Esteemed Admissions Panel of McMaster U...    1.0   \n",
       "4    4.0  Dear Admissions Committee of the University of...    1.0   \n",
       "\n",
       "                                           Clean_sop  \\\n",
       "0  to admissions committee university british col...   \n",
       "1  admission committee university toronto greetin...   \n",
       "2  dear admissions committee mcgill university my...   \n",
       "3  to esteemed admissions panel mcmaster universi...   \n",
       "4  dear admissions committee university toronto m...   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [to, admissions, committee, university, britis...   \n",
       "1  [admission, committee, university, toronto, gr...   \n",
       "2  [dear, admissions, committee, mcgill, universi...   \n",
       "3  [to, esteemed, admissions, panel, mcmaster, un...   \n",
       "4  [dear, admissions, committee, university, toro...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  to admission committee university british colu...  \n",
       "1  admission committee university toronto greetin...  \n",
       "2  dear admission committee mcgill university my ...  \n",
       "3  to esteem admission panel mcmaster university ...  \n",
       "4  dear admission committee university toronto my...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lemmatized_text'] = df['tokenized_text'].apply(lambda x: pos_mapping_lemmatization(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a9c820",
   "metadata": {},
   "source": [
    "#### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef2e5836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e5e98342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['lemmatized_text'], df['Label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5e98342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset: 1150\n",
      "Training Set: 920\n",
      "Validation Set: 230\n"
     ]
    }
   ],
   "source": [
    "# Train and Validation set distribution of the dataset\n",
    "lenSOP = df['lemmatized_text'].count()\n",
    "print(f'Total Dataset: {lenSOP}')\n",
    "print(f'Training Set: {len(X_train)}')\n",
    "print(f'Validation Set: {len(X_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c186f",
   "metadata": {},
   "source": [
    "RandomForestClassifier using CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2e74fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data: (920, 4598)\n"
     ]
    }
   ],
   "source": [
    "# Using Count Vectorizer to convert text to vectors\n",
    "vectorizer_lda = CountVectorizer(min_df=2)  # Adjust min_df as needed\n",
    "vectorizer_lda.fit(X_train)\n",
    "X_train_cv = vectorizer_lda.transform(X_train)\n",
    "X_val_cv = vectorizer_lda.transform(X_val)\n",
    "print(f'Shape of train data: {X_train_cv.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "91e75df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acquire' 'acquirement' 'acquisition' 'across' 'acrosstheboard' 'act'\n",
      " 'action' 'actionable' 'active' 'actively' 'activist' 'acton' 'actuarial'\n",
      " 'actuary' 'actuate' 'actuation' 'acumen' 'acute' 'adani' 'adaptability']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer_lda.get_feature_names_out()[100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a31bc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier using CountVectorizer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.913043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.944223</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.936759</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.940476</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Training Set  Validation Set\n",
       "Accuracy       0.934783        0.913043\n",
       "Precision      0.944223        0.920000\n",
       "Recall         0.936759        0.920000\n",
       "F1_score       0.940476        0.920000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Training and Prediction for Count Vectorizer using random forest classifier\n",
    "clf_cv = RandomForestClassifier(max_depth=2, max_leaf_nodes=3, n_estimators=20, class_weight={0:100, 1:60})\n",
    "clf_cv.fit(X_train_cv, y_train)\n",
    "y_pred_cv_train = clf_cv.predict(X_train_cv)\n",
    "y_pred_cv_val = clf_cv.predict(X_val_cv)\n",
    "\n",
    "# Model Evaluation for RFC\n",
    "metrics = {'Training Set':\n",
    "           {'Accuracy':accuracy_score(y_train, y_pred_cv_train),\n",
    "            'Precision':precision_score(y_train, y_pred_cv_train),\n",
    "            'Recall':recall_score(y_train, y_pred_cv_train),\n",
    "            'F1_score':f1_score(y_train, y_pred_cv_train)},\n",
    "           \n",
    "           'Validation Set':\n",
    "           {'Accuracy':accuracy_score(y_val, y_pred_cv_val),\n",
    "            'Precision':precision_score(y_val, y_pred_cv_val),\n",
    "            'Recall':recall_score(y_val, y_pred_cv_val),\n",
    "            'F1_score':f1_score(y_val, y_pred_cv_val)}\n",
    "            }\n",
    "\n",
    "metricDf = pd.DataFrame.from_dict(metrics)\n",
    "print('RandomForestClassifier using CountVectorizer')\n",
    "display(metricDf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320622f7",
   "metadata": {},
   "source": [
    "RandomForestClassifier using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32914585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data: (920, 6078)\n"
     ]
    }
   ],
   "source": [
    "# Using TF-IDF vectorizer for converting text to vectors\n",
    "vectorizer_lsa = TfidfVectorizer()\n",
    "vectorizer_lsa.fit(X_train)\n",
    "X_train_tfidf = vectorizer_lsa.transform(X_train)\n",
    "X_val_tfidf = vectorizer_lsa.transform(X_val)\n",
    "print(f'Shape of train data: {X_train_tfidf.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "942071bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acceptation' 'accepted' 'access' 'accessibility' 'accessible'\n",
      " 'accession' 'acclaim' 'accolade' 'accommodate' 'accompaniment'\n",
      " 'accompany' 'accomplish' 'accomplished' 'accomplishment' 'accord'\n",
      " 'accost' 'account' 'accounting' 'accredit' 'ace']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer_lsa.get_feature_names_out()[100:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0f075ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier using TfidfVectorizer and TruncatedSVD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.865217</td>\n",
       "      <td>0.873913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.965854</td>\n",
       "      <td>0.961538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.864629</td>\n",
       "      <td>0.873362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Training Set  Validation Set\n",
       "Accuracy       0.865217        0.873913\n",
       "Precision      0.965854        0.961538\n",
       "Recall         0.782609        0.800000\n",
       "F1_score       0.864629        0.873362"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Training and Prediction for TF-IDF using random forest classifier\n",
    "clf_tfidf = RandomForestClassifier(max_depth=2, max_leaf_nodes=3, n_estimators=20, class_weight={0:100, 1:60})\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf_train = clf_tfidf.predict(X_train_tfidf)\n",
    "y_pred_tfidf_val = clf_tfidf.predict(X_val_tfidf)\n",
    "\n",
    "# Model Evaluation for TF-IDF\n",
    "metrics = {'Training Set':\n",
    "           {'Accuracy':accuracy_score(y_train, y_pred_tfidf_train),\n",
    "            'Precision':precision_score(y_train, y_pred_tfidf_train),\n",
    "            'Recall':recall_score(y_train, y_pred_tfidf_train),\n",
    "            'F1_score':f1_score(y_train, y_pred_tfidf_train)},\n",
    "           \n",
    "           'Validation Set':\n",
    "           {'Accuracy':accuracy_score(y_val, y_pred_tfidf_val),\n",
    "            'Precision':precision_score(y_val, y_pred_tfidf_val),\n",
    "            'Recall':recall_score(y_val, y_pred_tfidf_val),\n",
    "            'F1_score':f1_score(y_val, y_pred_tfidf_val)}\n",
    "            }\n",
    "\n",
    "metricDf = pd.DataFrame.from_dict(metrics)\n",
    "print('RandomForestClassifier using TfidfVectorizer and TruncatedSVD')\n",
    "display(metricDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71535596",
   "metadata": {},
   "source": [
    "##### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a9e8ba2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m rf_clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m     12\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(rf_clf, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tfidf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m     17\u001b[0m best_rf_clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    730\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 732\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    736\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1089\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\nlp\\lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Using Hyperparameter tuning to find best values for the random forest classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [25, 50, 100, 150],\n",
    "    'max_depth': [3, 6, 9, 20, 30],\n",
    "    'max_leaf_nodes': [3, 6, 9, 20, 30]\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(rf_clf, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "best_rf_clf = RandomForestClassifier(**grid_search.best_params_)\n",
    "best_rf_clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Training set score:\", best_rf_clf.score(X_train_tfidf, y_train))\n",
    "print(\"Test set score:\", best_rf_clf.score(X_val_tfidf, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a04a20",
   "metadata": {},
   "source": [
    "RandomForestClassifier using CountVectorizer and LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ee5e4f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dimensionality reduction of train data: (920, 3000)\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of topics for LDA\n",
    "num_topics_lda = 3000  # You can adjust this number as needed\n",
    "lda = LatentDirichletAllocation(n_components=num_topics_lda, random_state=42)\n",
    "lda.fit(X_train_cv)  # Fit LDA model for training data\n",
    "X_train_topics_lda = lda.fit_transform(X_train_cv)\n",
    "X_val_topics_lda = lda.transform(X_val_cv)  # Transform test data into topic distributions\n",
    "print(f'Shape after dimensionality reduction of train data: {X_train_topics_lda.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2544933c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Prediction for LDA\n",
    "clf_lda = RandomForestClassifier(max_depth=20, max_leaf_nodes=30, n_estimators=25, class_weight={0:100, 1:60})\n",
    "clf_lda.fit(X_train_topics_lda, y_train)\n",
    "y_pred_lda_train = clf_lda.predict(X_train_topics_lda)\n",
    "y_pred_lda_val = clf_lda.predict(X_val_topics_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5b274ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier using CountVectorizer and LatentDirichletAllocation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.778261</td>\n",
       "      <td>0.917391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.980892</td>\n",
       "      <td>0.889706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.751220</td>\n",
       "      <td>0.927203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Training Set  Validation Set\n",
       "Accuracy       0.778261        0.917391\n",
       "Precision      0.980892        0.889706\n",
       "Recall         0.608696        0.968000\n",
       "F1_score       0.751220        0.927203"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Evaluation for LDA\n",
    "metrics = {'Training Set':\n",
    "           {'Accuracy':accuracy_score(y_train, y_pred_lda_train),\n",
    "            'Precision':precision_score(y_train, y_pred_lda_train),\n",
    "            'Recall':recall_score(y_train, y_pred_lda_train),\n",
    "            'F1_score':f1_score(y_train, y_pred_lda_train)},\n",
    "           \n",
    "           'Validation Set':\n",
    "           {'Accuracy':accuracy_score(y_val, y_pred_lda_val),\n",
    "            'Precision':precision_score(y_val, y_pred_lda_val),\n",
    "            'Recall':recall_score(y_val, y_pred_lda_val),\n",
    "            'F1_score':f1_score(y_val, y_pred_lda_val)}\n",
    "            }\n",
    "\n",
    "metricDf = pd.DataFrame.from_dict(metrics)\n",
    "print('RandomForestClassifier using CountVectorizer and LatentDirichletAllocation')\n",
    "display(metricDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81c93e5",
   "metadata": {},
   "source": [
    "RandomForestClassifier using TfidfVectorizer and TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88e787ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dimensionality reduction of train data: (920, 920)\n"
     ]
    }
   ],
   "source": [
    "# Specify the number of components for LSA\n",
    "num_components_lsa = 3000  # You can adjust this number as needed\n",
    "lsa = TruncatedSVD(n_components=num_components_lsa, random_state=42)\n",
    "lsa.fit(X_train_tfidf)  # Fit LSA model for training data\n",
    "X_train_components_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "X_val_components_lsa = lsa.transform(X_val_tfidf)  # Transform test data into components\n",
    "print(f'Shape after dimensionality reduction of train data: {X_train_components_lsa.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "35ef7dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training and Prediction for LSA\n",
    "clf_lsa = RandomForestClassifier(max_depth=20, max_leaf_nodes=30, n_estimators=25, class_weight={0:100, 1:10})\n",
    "clf_lsa.fit(X_train_components_lsa, y_train)\n",
    "y_pred_lsa_train = clf_lsa.predict(X_train_components_lsa)\n",
    "y_pred_lsa_val = clf_lsa.predict(X_val_components_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1fcb3821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier using TfidfVectorizer and TruncatedSVD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Set</th>\n",
       "      <th>Validation Set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.995652</td>\n",
       "      <td>0.856522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.933962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.992095</td>\n",
       "      <td>0.792000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.996032</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Training Set  Validation Set\n",
       "Accuracy       0.995652        0.856522\n",
       "Precision      1.000000        0.933962\n",
       "Recall         0.992095        0.792000\n",
       "F1_score       0.996032        0.857143"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Evaluation for LSA\n",
    "metrics = {'Training Set':\n",
    "           {'Accuracy':accuracy_score(y_train, y_pred_lsa_train),\n",
    "            'Precision':precision_score(y_train, y_pred_lsa_train),\n",
    "            'Recall':recall_score(y_train, y_pred_lsa_train),\n",
    "            'F1_score':f1_score(y_train, y_pred_lsa_train)},\n",
    "           \n",
    "           'Validation Set':\n",
    "           {'Accuracy':accuracy_score(y_val, y_pred_lsa_val),\n",
    "            'Precision':precision_score(y_val, y_pred_lsa_val),\n",
    "            'Recall':recall_score(y_val, y_pred_lsa_val),\n",
    "            'F1_score':f1_score(y_val, y_pred_lsa_val)}\n",
    "            }\n",
    "\n",
    "metricDf = pd.DataFrame.from_dict(metrics)\n",
    "print('RandomForestClassifier using TfidfVectorizer and TruncatedSVD')\n",
    "display(metricDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8c45edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling ml model, vectorizer and dimensionality reduction models\n",
    "import pickle\n",
    "\n",
    "f = open('Deployment\\Models\\\\vectorizerTFIDF.pkl', 'wb')\n",
    "pickle.dump(vectorizer_lsa, f)\n",
    "f.close()\n",
    "\n",
    "f = open('Deployment\\Models\\\\vectorizerCV.pkl', 'wb')\n",
    "pickle.dump(vectorizer_lda, f)\n",
    "f.close()\n",
    "\n",
    "f = open('Deployment\\Models\\LDA.pkl', 'wb')\n",
    "pickle.dump(lda, f)\n",
    "f.close()\n",
    "\n",
    "f = open('Deployment\\Models\\TSVD.pkl', 'wb')\n",
    "pickle.dump(lsa, f)\n",
    "f.close()\n",
    "\n",
    "f = open('Deployment\\Models\\RF_CV_LDA.pkl', 'wb')\n",
    "pickle.dump(clf_lda, f)\n",
    "f.close()\n",
    "\n",
    "f = open('Deployment\\Models\\RF_TFIDF_TSVD.pkl', 'wb')\n",
    "pickle.dump(clf_lsa, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84c2802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
